# -*- coding: utf-8 -*-
"""PromoCast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fgc-5jzWszrItBmeDIuT3lWkLmNJofos
"""

#Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

#Load CSV Files with safe dtype handling
train = pd.read_csv('/content/train.csv', low_memory=False)
store = pd.read_csv('/content/store.csv', low_memory=False)

#Merge Datasets
data = pd.merge(train, store, how='left', on='Store')

#Data Cleaning
#only open stores with non-zero sales
data = data[(data['Open'] == 1) & (data['Sales'] > 0)]

#Feature Engineering from 'Date'
data['Date'] = pd.to_datetime(data['Date'])
data['Year'] = data['Date'].dt.year
data['Month'] = data['Date'].dt.month
data['Day'] = data['Date'].dt.day
data['WeekOfYear'] = data['Date'].dt.isocalendar().week
data['DayOfWeek'] = data['Date'].dt.dayofweek

#Advanced Feature Engineering
data['IsWeekend'] = (data['DayOfWeek'] >= 5).astype(int)
data['PromoSchoolHoliday'] = data['Promo'] * data['SchoolHoliday']

#Drop Unused Columns
data = data.drop(columns=['Date', 'Customers'])  # 'Customers' is not available at prediction time

#Handle Missing Values
data = data.fillna(0)

#Encode Categorical Variables
data = pd.get_dummies(data, columns=['StoreType', 'Assortment', 'StateHoliday'], drop_first=True)

#Sample 20% of the dataset
data = data.sample(frac=0.2, random_state=42).reset_index(drop=True)

#Define Features and Target
X = data.drop(columns=['Sales'])
y = data['Sales']

#Ensure X is numeric
X = X.apply(pd.to_numeric, errors='coerce')
X = X.fillna(0)

#Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Random Forest Base Model
base_model = RandomForestRegressor(random_state=42, n_jobs=-1)

#Hyperparameter Tuning using RandomizedSearchCV
param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

rf_random = RandomizedSearchCV(
    estimator=base_model,
    param_distributions=param_grid,
    n_iter=20,
    cv=3,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

rf_random.fit(X_train, y_train)

#Best Model after Tuning
model = rf_random.best_estimator_

#Model Training is done, now Predictions
y_pred = model.predict(X_test)

#Model Evaluation
print("MAE:", mean_absolute_error(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("RÂ² Score:", r2_score(y_test, y_pred))

#Cross-Validation Score
cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')
print("Cross-Validated MAE:", -np.mean(cv_scores))

#Actual vs Predicted Scatter Plot
plt.figure(figsize=(8,5))
plt.scatter(y_test[:1000], y_pred[:1000], alpha=0.5)
plt.xlabel("Actual Sales")
plt.ylabel("Predicted Sales")
plt.title("Actual vs Predicted Sales")
plt.grid(True)
plt.show()

#Residual Plot
residuals = y_test - y_pred
plt.figure(figsize=(8,5))
sns.histplot(residuals, kde=True)
plt.title("Residual Distribution")
plt.xlabel("Residual (Actual - Predicted)")
plt.grid(True)
plt.show()

#Feature Importance
importances = pd.Series(model.feature_importances_, index=X.columns)
importances.nlargest(10).plot(kind='barh')
plt.title("Top 10 Important Features")
plt.show()